---
title: "lab_notebook"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lab_notebook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r Rsetup}
library(Rpowerlawnoise)
pycolors <- c(blue=   rgb(0.00, 0.00, 1.00, 0.8),
              orange= rgb(1.00, 0.50, 0.00, 0.8),
              green=  rgb(0.00, 0.90, 0.00, 0.8),
              darkred=rgb(0.65, 0.00, 0.00, 0.8))
```

# 2020 July 29

## Introduction

Power law noise is a time series where the energy at different frequencies is a
function of the frequency raised to a power. True red noise has a power of -2,
true white noise has a power of 0, and true violet noise has a power of 2. Pink
noise has a power between -2 and 0, exclusive. Blue noise has a power between 0
and 2, exclusive. There are many ways to generate power law noise. A
particularly useful one uses an autoregressive model where the weights given to
previous values of the time series are a function of the power and a decreasing
function of the time lag to the term. Some powers have a fixed number of non-
zero terms. White noise, for example, has no non-zero terms, and true red noise
has only one. All other powers have a potentially infinite number of terms. The
actual number of terms in an autoregressive model is its degree. Obviously,
there are diminishing returns to increasing the degree of a model. This project
is aimed at finding a rule for choosing how big a model's degree should be. The
rule will be quantified according to the difference that one is willing to
tolerate between the observed spectral slope and the expected slope of the
spectrum of the time series.

There are three crucial parameters that I want to examine:

$\alpha$
: The power of the power law. When the spectral energy at a frequency $f$ is
$S(f)$ then $S(f) \propto f^{\alpha}$.

$N$
: The number of samples in the time series

$K$
: The degree of the autoregressive model

## Setup

I have actually already written a ton of code for this project, both in R and
Python. However, I didn't really have it set up as a coherent repository. I'm
trying to build on my recent experience with writing a modern, user-friendly R
package to make both R Python packages that readers of the subsequent paper can
use for their own ends.

### the first snag
I had to install the R package `reticulate` before I could compile this
vignette.

### gathering data
I ran a simulation to create time series with many combinations of $\alpha$,
$N$, and $K$. I used $\alpha$ values from -2 to 2, spaced by 0.01. I used $K$
values from 1 to 50. The $N$ values were powers of two: 32, 64, 128, 256, 512,
1024, 2048, and 4096. The simulation was in Python and its code will be in the
"pypowerlawnoise" package when I get around to assembling it. I saved the
results in a csv file, then loaded it into R and used `usethis::use_data` to
save it as internal data. It's really big. We'll see what GitHub thinks.

### the second snag
I can't use internal data in the vignette. That sucks, but I'm not putting this
package on CRAN, anyway. Just deleting "R/sysdata.rda" seemed to work fine.

## Comparing the Observed Slope to the Actual Power

Here's what the results look like for a couple of different degrees when the
length of the time series is 256.

```{r, fig.dim=c(6, 6), out.width="95%"}
plot(0, 0, type="n", las=1,
     xlab=expression(alpha), ylab="Observed slope",
     xlim=c(-2, 2), ylim=c(-2, 2))
grid()
sample.degrees <- c(2, 8, 20, 50)
invisible(with(subset(results, Size==256),
          lapply(seq_along(sample.degrees), function(i) {
              f <- Degree==sample.degrees[i]
              points(Slope[f] ~ Power[f],
              pch=".", col=pycolors[i])
          })))
legend("topleft", legend=sample.degrees, title="Model Degree",
       pch=16, col = pycolors, bty="n")
abline(0, 1, lwd=2, lty=2)
```
As you can see, there is a nearly one-to-one relationship, but it's not
perfect. When $K$ is small, the observed slopes are more dispersed around the
expected power. Even when the degree is large, there is a poorer fit when
$\alpha$ is far from zero. It does look like a straight-line relationship at
each degree though, so I'm going to look at the relationships between $\alpha$,
$K$, and $N$ and the slope of the line that predicts thet observed fit from the
actual power.

```{r, fig.dim=c(6, 6), out.width="95%"}
sizes <- sort(unique(results$Size))
degrees <- sort(unique(results$Degree))
slope.info <- expand.grid(Size=sizes, Degree=degrees)
slope.models <- lapply(seq_len(nrow(slope.info)),
                             function(i) {
							     lm(Slope ~ Power - 1, results,
							        subset=Size == slope.info$Size[i] &
										   Degree == slope.info$Degree[i])
                             })
slope.info$Metaslope <- sapply(slope.models, coef)
slope.info$SS.Residuals <- sapply(slope.models, function(m)sum(residuals(m)^2))
size.colors <- rainbow(length(sizes), alpha=0.75)
plot(Metaslope ~ Degree, slope.info,
	 type="n", las=1,
     xlab="Degree of Model", ylab="Metaslope between Slope and Power")
invisible(lapply(seq_along(sizes), function(i){
    points(Metaslope ~ Degree, slope.info, subset=Size==sizes[i],
	       type="b", lty=1, col=size.colors[i], pch=16)
}))
legend("bottomright", legend=sizes, title="Length of Time Series",
       pch=16, col=size.colors[seq_along(sizes)], lty=1, bty="n")
```
As you can see from the image, it seems that the metaslope slope approaches
a limit as the degree increases, and the value of that limit approaches
something like 0.87 as length of the time series increases. Look at the red and
yellow traces. There is a better (closer to one) metaslope with small $K$
when $N$ is also small. This looks like differential metaslopes where the
asymptote is a concave-down, increasing function of $N$, and the rate of
approach to the asymptote is a concave up, decreasing function of $K$ and $N.
Time for `sympy`!

```{python algebra}
import sympy as sp
z, w, K, N = sp.symbols("z w K N")
R = sp.Function("R")
Rp = sp.diff(R(K), K)
Rpp = sp.diff(Rp, K)
U = Rpp + 2*z*w*Rp + w**2 * R(K)
print(U)
V = sp.dsolve(U, R(K))
sp.latex(V)
```
Since the answer has the form $c\exp(-bK) + a$, we can use `nls` to find $c$,
$b$, and $a$ for each value of $N$.
Actually, it's a second-order diffeq:
$R{\left(K \right)} =
\left(C_{1} e^{- K w \sqrt{z^{2} - 1}} +
C_{2} e^{K w \sqrt{z^{2} - 1}}\right) e^{- K w z}$

# 2020 July 29

__OF COURSE__ it _LOOKS_ like a damped oscillator, but then the math _DOES NOT_
match. So, I looked at plotting things some other ways. First, I looked at how
a model's degree ($K$) affects the slope ($m$?) that relates desired Power
($\alpha$) and the observed spectral slope ($a$?).

```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ Degree, slope.info,
     las=1, log="x",
	 pch=1, cex=3/2, col=size.colors[factor(Size)],
	 xlab=expression(K), ylab="Metaslope between Slope and Power")
legend("bottomright", legend=sizes, title=expression(N),
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Squinting at this picture makes it clear that the metaslope has a local
maximum value for each size. They seem crammed up near $K=64$, though. Perhaps
if we looked at the metaslope compared to the ratio of degree to time series
length.
```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ I(Degree/Size), slope.info,
     las=1, log="x",
	 pch=1, cex=3/2, col=size.colors[factor(Size)],
	 xlab=expression(K/N), ylab="Metaslope between Slope and Power")
legend("bottomright", legend=sizes, title=expression(N),
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Look at that! They're starting to line up. They're still not there yet, though.
It looks to me that they're all peaking at about where the degree is the same
as the square root of the length of the time series. Visually checking:
```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ I(Degree/sqrt(Size)), slope.info,
     las=1, log="x",
	 pch=1, cex=3/2, col=size.colors[factor(Size)],
	 xlab=expression(K/sqrt(N)), ylab="Metaslope between Slope and Power")
abline(v=1)
legend("bottomright", legend=sizes, title=expression(N),
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Personally, I find this picture very convincing. In the general case, the best
degree for a model is the square root of the desired time series' length. The
next step is to put a framework around it that is more rigorous than "look at
how obvious it is in the graph!" The first approach that springs to mind is to
graph these special cases and see what happens.
