---
title: "lab_notebook"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lab_notebook}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r Rsetup}
library(Rpowerlawnoise)
pycolors <- c(blue=   rgb(0.00, 0.00, 1.00, 0.8),
              orange= rgb(1.00, 0.50, 0.00, 0.8),
              green=  rgb(0.00, 0.90, 0.00, 0.8),
              darkred=rgb(0.65, 0.00, 0.00, 0.8))
results <- read.csv("../data-raw/power_law_output.csv")
```

# 2020 July 29

## Introduction

Power law noise is a time series where the energy at different frequencies is a
function of the frequency raised to a power. True red noise has a power of -2,
true white noise has a power of 0, and true violet noise has a power of 2. Pink
noise has a power between -2 and 0, exclusive. Blue noise has a power between 0
and 2, exclusive. There are many ways to generate power law noise. A
particularly useful one uses an autoregressive model where the weights given to
previous values of the time series are a function of the power and a decreasing
function of the time lag to the term. Some powers have a fixed number of non-
zero terms. White noise, for example, has no non-zero terms, and true red noise
has only one. All other powers have a potentially infinite number of terms. The
actual number of terms in an autoregressive model is its degree. Obviously,
there are diminishing returns to increasing the degree of a model. This project
is aimed at finding a rule for choosing how big a model's degree should be. The
rule will be quantified according to the difference that one is willing to
tolerate between the observed spectral slope and the expected slope of the
spectrum of the time series.

There are three crucial parameters that I want to examine:

$\alpha$
: The power of the power law. When the spectral energy at a frequency $f$ is
$S(f)$ then $S(f) \propto f^{\alpha}$.

$N$
: The number of samples in the time series

$K$
: The degree of the autoregressive model

## Setup

I have actually already written a ton of code for this project, both in R and
Python. However, I didn't really have it set up as a coherent repository. I'm
trying to build on my recent experience with writing a modern, user-friendly R
package to make both R and Python packages that readers of the subsequent paper
can use for their own ends.

### the first snag
I had to install the R package `reticulate` before I could compile this
vignette.

### gathering data
I ran a simulation to create time series with many combinations of $\alpha$,
$N$, and $K$. I used $\alpha$ values from -2 to 2, spaced by 0.01. I used $K$
values from 1 to 50. The $N$ values were powers of two: 32, 64, 128, 256, 512,
1024, 2048, and 4096. The simulation was in Python and its code will be in the
"pypowerlawnoise" package when I get around to assembling it. I saved the
results in a csv file, then loaded it into R and used `usethis::use_data` to
save it as internal data. It's really big. We'll see what GitHub thinks.

### the second snag
I can't use internal data in the vignette. That sucks, but I'm not putting this
package on CRAN, anyway. Just deleting "R/sysdata.rda" seemed to work fine.

## Comparing the Observed Slope to the Actual Power

Here's what the results look like for a couple of different degrees when the
length of the time series is 256.

```{r, fig.dim=c(6, 6), out.width="95%"}
plot(0, 0, type="n", las=1,
     xlab=expression(alpha), ylab="Observed slope",
     xlim=c(-2, 2), ylim=c(-2, 2))
grid()
sample.degrees <- c(2, 8, 20, 50)
invisible(with(subset(results, Size==256),
          lapply(seq_along(sample.degrees), function(i) {
              f <- Degree==sample.degrees[i]
              points(Slope[f] ~ Power[f],
              pch=".", col=pycolors[i])
          })))
legend("topleft", legend=sample.degrees, title="Model Degree",
       pch=16, col = pycolors, bty="n")
abline(0, 1, lwd=2, lty=2)
```
As you can see, there is a nearly one-to-one relationship, but it's not
perfect. When $K$ is small, the observed slopes are more dispersed around the
expected power. Even when the degree is large, there is a poorer fit when
$\alpha$ is far from zero. It does look like a straight-line relationship at
each degree though, so I'm going to look at the relationships between $\alpha$,
$K$, and $N$ and the slope of the line that predicts thet observed fit from the
actual power.

```{r, fig.dim=c(6, 6), out.width="95%"}
sizes <- sort(unique(results$Size))
size.colors <- rainbow(length(sizes), alpha=0.75)
degrees <- sort(unique(results$Degree))
slope.info <- expand.grid(Size=sizes, Degree=degrees)
slope.models <- lapply(seq_len(nrow(slope.info)),
                             function(i) {
                                 lm(Slope ~ Power - 1, results,
                                    subset=Size == slope.info$Size[i] &
                                           Degree == slope.info$Degree[i])
                             })
slope.info$Metaslope <- sapply(slope.models, coef)
slope.info$SS.Residuals <- sapply(slope.models, function(m)sum(residuals(m)^2))
plot(Metaslope ~ Degree, slope.info,
     type="n", las=1,
     xlab="Degree of Model", ylab="Metaslope between Slope and Power")
invisible(lapply(seq_along(sizes), function(i){
    points(Metaslope ~ Degree, slope.info, subset=Size==sizes[i],
           type="b", lty=1, col=size.colors[i], pch=16)
}))
legend("bottomright", legend=sizes, title="Length of Time Series",
       pch=16, col=size.colors[seq_along(sizes)], lty=1, bty="n")
```
As you can see from the image, it seems that the metaslope slope approaches
a limit as the degree increases, and the value of that limit approaches
something like 0.87 as length of the time series increases. Look at the red and
yellow traces. There is a better (closer to one) metaslope with small $K$
when $N$ is also small. This looks like differential metaslopes where the
asymptote is a concave-down, increasing function of $N$, and the rate of
approach to the asymptote is a concave up, decreasing function of $K$ and $N.
Time for `sympy`!

```{python algebra}
import sympy as sp
z, w, K, N = sp.symbols("z w K N")
R = sp.Function("R")
Rp = sp.diff(R(K), K)
Rpp = sp.diff(Rp, K)
U = Rpp + 2*z*w*Rp + w**2 * R(K)
print(U)
V = sp.dsolve(U, R(K))
sp.latex(V)
```
Since the answer has the form $c\exp(-bK) + a$, we can use `nls` to find $c$,
$b$, and $a$ for each value of $N$.
Actually, it's a second-order diffeq:
$$
    R{\left(K \right)} =
    \left(C_{1} e^{- K w \sqrt{z^{2} - 1}} +
    C_{2} e^{K w \sqrt{z^{2} - 1}}\right) e^{- K w z}
$$

# 2020 July 29

__OF COURSE__ it _LOOKS_ like a damped oscillator, but then the math _DOES NOT_
match. So, I looked at plotting things some other ways. First, I looked at how
a model's degree ($K$) affects the slope ($m$?) that relates desired Power
($\alpha$) and the observed spectral slope ($a$?).

```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ Degree, slope.info,
     las=1, log="x",
     pch=1, cex=3/2, col=size.colors[factor(Size)],
     xlab=expression(K), ylab="Metaslope between Slope and Power")
legend("bottomright", legend=sizes, title=expression(N),
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Squinting at this picture makes it clear that the metaslope has a local
maximum value for each size. They seem crammed up near $K=64$, though. Perhaps
if we looked at the metaslope compared to the ratio of degree to time series
length.
```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ I(Degree/Size), slope.info,
     las=1, log="x",
     pch=1, cex=3/2, col=size.colors[factor(Size)],
     ylim=c(0.65, 0.9),
     xlab=expression(K/N), ylab="Metaslope between Slope and Power")
legend("bottomright", legend=sizes, title=expression(N),
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Look at that! They're starting to line up. They're still not there yet, though.
It looks to me that they're all peaking at about where the degree is the same
as the square root of the length of the time series. Visually checking:
```{r fig.dim=c(6, 6), out.width="95%"}
plot(Metaslope ~ I(Degree/sqrt(Size)), slope.info,
     las=1, log="x",
     pch=1, cex=3/2, col=size.colors[factor(Size)],
     xlim=10^c(-1, 1), ylim=c(0.84, 0.9),
     xlab=expression(K/sqrt(N)), ylab="Metaslope between Slope and Power")
abline(v=1)
legend("topleft", legend=sizes, title=expression(N),
       ncol=2,
       pch=1, col=size.colors, pt.cex=3/2, bty="n")
```
Personally, I find this picture very convincing. In the general case, the best
degree for a model is the square root of the desired time series' length. The
next step is to put a framework around it that is more rigorous than "look at
how obvious it is in the graph!" The first approach that springs to mind is to
graph these special cases and see what happens.

# 2020 August 11

Well, it's been a while, but that's OK because I've made a lot of improvements
in the Python code. I also neglected to update the vignette on several days,
mostly because of crippling depression. However, I did make progress in the
analysis on those days.

# 2020 August 12

The python code does seem to work well. One improvement to the new version is
that it can handle models with degree of zero. Those are all white noise, by
definition, which makes it pretty clear that there should be two fits, one for
reddish noise, and one for blueish noise. Furthermore, neither of those should
necessarily be forced to go through the origin.

# 2020 August 13

The python code does work, and I generated a preliminary data set, this time
with different degrees for different sizes, with the degrees centered on the
square root of the length of the time series' sizes. Thee vignette approach
above does not work on data where there are different degrees for different
sizes. It might be tweakable with error catching rather than different logic,
but I can't do that right now.

# 2020 August 14

Time to start wrangling the centered data. Inspecting it revealed that there is
some log-log relationship between degree ratio and the difference between slope
and power. However, size does seem to dominate things. Here are six different
powers, showing the relationship between the absolute slope error and both size
and degree ratio:

```{r fig.dim=c(6, 6), out.width="95%"}
results <- read.csv("../data-raw/centered.csv")
results$Degree.Ratio <- with(results, Degree/sqrt(Size))
results$Slope.Error <- with(results, abs(Slope - Power))
sizes <- sort(unique(results$Size))
size.colors <- rainbow(length(sizes), alpha=0.75)
degrees <- sort(unique(results$Degree))
ratios <- 10^seq(-log10(8), log10(8), length.out=50)
par(mfrow=c(2,3));
invisible(sapply(c(-2, -1.32, -0.68, 0.68, 1.32, 2), function(alpha){
    with(subset(results, round(Power, 2) == alpha), {
    tmp <- lm(log10(Slope.Error) ~
              log10(Size)*(log10(Degree.Ratio) + I(log10(Degree.Ratio)^2)))
    plot(Slope.Error ~ Degree.Ratio,
         pch=1, cex=1/4, col=size.colors[factor(Size)],
         las=1,
         log="xy", ylim=c(0.0002, 2),
         xlab="Degree Ratio",
         ylab="Slope Error",
         main=bquote(alpha==.(alpha)))
    grid()
    sapply(seq_along(sizes), function(i){
        x <- ratios
        y <- 10^predict(tmp, expand.grid(Size=sizes[i], Degree.Ratio=ratios));
        lines(x, y, col=size.colors[i], lty=1)
    })
})}))
```

Let's look at the coefficients of these models as the power changes
```{r fig.dim=c(6, 6), out.width="95%"}
alphas <- sort(unique(results$Power))
result.error.models <- lapply(alphas, function(alpha){
    lm(log10(Slope.Error) ~
       log10(Size)*(log10(Degree.Ratio) + I(log10(Degree.Ratio)^2)),
       results,
       subset=Power==alpha)
})
par(mfrow=c(2, 3))
invisible(sapply(1:5, function(j){
    plot(alphas,
         sapply(result.error.models, function(m)coef(m)[j]),
         ylab=ifelse(j==1,
                     "Intercept",
                     attr(terms(result.error.models[[1]]),
                          "term.labels")[j-1]))
}))
```

That makes it look to me that the only really significant terms are the
intercept and the logarithm of size. I'm going to look at the residuals then.
```{r fig.dim=c(6, 6), out.width="95%"}
size.residual.models <- lapply(alphas, function(alpha){
    lm(log10(Slope.Error) ~ log10(Size), results, subset=Power==alpha)
})
results$Size.Residual.Error <- NA
for(i in seq_along(alphas)){
    f <- results$Power == alphas[i]
    results$Size.Residual.Error[f] <- log10(results$Slope.Error[f])
    - predict(size.residual.models[[i]], results[f,])
}

plot(Size.Residual.Error ~ Power, results,
     pch=1, col=size.colors[factor(Size)], cex=1/4,
     las=1,
     xlab=expression(alpha),
     ylab="Size-corrected Slope Error")

```

# 2020 August 16

I think I figured it out but, of course, I didn't write stuff down as I was
making progress. I'm actually writing this the next day. I refined the approach
some the next day, so I'll pick up there.

# 2020 August 17

I was still not satisfied with my analyses of how slope error changed with size
and degree. In particular, I didn't like how the residual variance was larger
for small sizes. I did a little Googling, and found
[this site](https://fukamilab.github.io/BIO202/03-C-heterogeneity.html), which
explains how to use the `gls` function to account for several different kinds
of variance heterogeneity. It turns out that the best variance model is
`varPower`, with a formula of `~ Size`.

```{r fig.dim=c(6, 6), out.width="95%"}
library(nlme)
simple.form <- formula(Slope.Error ~ I(1/sqrt(Size)) + I(1/Degree.Ratio))
different.models <- list(
    "Power.Alone"=function(X){
        gls(simple.form, X, weights=varPower(form=~Size))
    },
    "Exponent"=function(X){
        gls(simple.form, X, weights=varExp(form=~Size))
    },
    "Const.Power"=function(X){
        gls(simple.form, X, weights=varConstPower(form=~Size))
    }
)
different.fits <- lapply(different.models, function(f){
    lapply(alphas,
           function(alpha)f(subset(results, Power==alpha)))
})
different.AICs <- data.frame(Power=alphas,
                             lapply(different.fits,
                                    function(ell)sapply(ell, AIC)))
plot(0, 0, type="n", las=1,
     xlab=expression(alpha), ylab="AIC",
     xlim=c(-2, 2), ylim=c(-8000, 1000))
grid()
invisible(sapply(2:4, function(j){
    points(different.AICs[, c(1, j)], cex=3, col=size.colors[2*(j-2)+1])
}))
legend("topleft",
       legend=names(different.AICs)[2:4],
       pch=1, col=size.colors[c(1, 3, 5)],
       bty="n", cex=4/3)
```

With any luck, this will show that `varPower` and `varConstPower` are both
better than `varExp`, and that `varPower` is better than `varConstPower`
because it's simpler.

Did you notice that I also slipped some new models in there? It now seem that
the complicated fits that I was using before are actually unnecessary. The key
insight is **Degree Ratio**, which is $\frac{K}{\sqrt{N}}$. The model becomes:
$$
    E(\alpha) = B_0(\alpha)
	+ \frac{B_1(\alpha)}{\sqrt{N}}
	+ \frac{B_2(\alpha)\sqrt{N}}{K}
$$

This has really nice properties. First, its limit is straightforward:
$$
    \lim_{K \to \infty}{E(\alpha)} = B_0(\alpha) + \frac{B_1(\alpha)}{\sqrt{N}}
$$

which means we can use epsilon-delta to look for a degree that is arbitrarily
close to this limit:
$$
    \lim_{\delta \to 0}{\frac{B_2(\alpha)\sqrt{N}}{K+\delta}} < \epsilon
$$

which simplifies to:
$$
  \begin{align}
    B_2(\alpha)\sqrt{N} & < \epsilon({K + \delta}) \\
	B_2(\alpha)\sqrt{N} & < \epsilon K + \epsilon\delta \\
	B_2(\alpha)\sqrt{N} & < \epsilon K \\
	\epsilon K & > B_2(\alpha)\sqrt{N} \\
	K &> \frac{B_2(\alpha)\sqrt{N}}{\epsilon}
  \end{align}
$$

The excellent thing about this is that it comes back to degree ratio: 
$$
    \frac{K}{\sqrt{N}} = \frac{B_2(\alpha)}{\epsilon}
$$

FINALLY, a univariate function on $\alpha$ that suggests an 'optimum'
degree relative to a time series' length!

```{r fig.dim=c(6, 6), out.width="95%"}
different.AICs$Coefficient <- sapply(different.fits$Power.Alone,
                                     function(mo)coef(mo)[3])
epsilons <- list("black"=0.01, "red"=0.005, "blue"=0.001) 
plot(0, 0, type="n", las=1,
     xlab=expression(alpha), ylab=expression(B[2](alpha)/epsilon),
	 xlim=c(-2, 2), ylim=c(0, 20))
grid()
abline(v=4/3, lty=2, lwd=2)
invisible(lapply(names(epsilons), function(n){
    points(ceiling(Coefficient/epsilons[[n]]) ~ Power, different.AICs,
	       cex=2, col=n)
}))
legend("topleft", bty="n",
       title=expression(epsilon),
	   legend=epsilons,
	   col=c(1, 2, 4), cex=3/2, pch=1)
```
This figure fills me with glee because I can see the features that I know we
need: $\check{K}(-2) = 1$, $\check{K}(0) = 0$, pink noise has a modest
optiumum, and blue noise has a (mostly) increasing optimum. This graph shows
how the relationship really breaks down around $\alpha > 4/3$, but that's OK
because the y-axis is really Degree Ratio, and _this data set doesn't actually
have any Degree Ratios greater than 8_! I am currently running a new BF'd up
data set with Degree Ratios between 1/20 and 20. Hopefully, this will give a
better fit to what goes on when \$alpha$ is large.

### Sanity check
Maybe the limit should be:
$$
  \lim_{\delta \to 0}{
      p(\alpha)+\frac{q(\alpha)}{\sqrt{N}}+\frac{r(\alpha)\sqrt{N}}{K} -
      p(\alpha)+\frac{q(\alpha)}{\sqrt{N}}+\frac{r(\alpha)\sqrt{N}}{K + \delta}
  } < \epsilon
$$

That simplifies to $$
  \begin{align}
    \lim_{\delta \to 0}{
        \frac{r(\alpha)\sqrt{N}}{K} - \frac{r(\alpha)\sqrt{N}}{K + \delta}
    } & < \epsilon \\
    \lim_{\delta \to 0}{
        r(\alpha)\sqrt{N}(K + \delta) - r(\alpha)\sqrt{N} K
    } & < K (K + \delta) \epsilon \\
    \lim_{\delta \to 0}{
        r(\alpha)\sqrt{N}K + r(\alpha)\sqrt{N}\delta - r(\alpha)\sqrt{N} K
    } & < K^2 \epsilon + K \delta \epsilon \\
    r(\alpha)\sqrt{N}K - r(\alpha)\sqrt{N} K & < K^2 \epsilon \\
    r(\alpha)\sqrt{N} - r(\alpha)\sqrt{N} & < K \epsilon \\
	0 & < K \epsilon
  \end{align}
$$

Nope! Whee!
